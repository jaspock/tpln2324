{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"T\u00e9cnicas de Procesamiento del Lenguaje Natural, 2023-2024","text":""},{"location":"#homework-assignments","title":"Homework Assignments","text":"<ul> <li>Assignments before the class of Jan 19, 2024: Read the materials related to speech recognition, linked in this section and then complete this test (deadline: 23:59 CET, Jan 18 2024).</li> <li>Assignments before the class of Jan 17, 2024: Read the materials related to full encoder-decoder transformer models, encoder-like transformers, large language models and multilingual models linked in this section and then complete this test (deadline: 23:59 CET, Jan 16 2024).</li> <li>Assignments before the class of Jan 10, 2024: Read the materials related to embeddings, neural networks and transformers, linked in this section and then complete this test (deadline: 23:59 CET, Jan 9 2024).</li> <li>Assignments before the class of Dec 20, 2023: This class will have two parts taught by different teachers; therefore, your assignments will deal with two different topics. Firstly, read the new contents of section Introduction to computational linguistics and natural language processing; after that, complete this test and read the article \"The Future of Computational Linguistics: On Beyond Alchemy\" here (Reading time: one hour and a half) (deadline: 23:59 CET, Dec 19 2023). Secondly, read the materials related to logistic regressors and PyTorch linked in this section and then complete this test (same deadline: 23:59 CET, Dec 19 2023).</li> <li>Assignments before the class of Dec 13, 2023: read the contents of section Introduction to computational linguistics and natural language processing; after that, complete this test (deadline: 23:59 CET, Dec 12 2023).</li> </ul>"},{"location":"#course-syllabus-and-rules","title":"Course Syllabus and Rules","text":"<p>These are the teaching materials for the course T\u00e9cnicas de Procesamiento del Lenguaje Natural, coordinated by professor Juan Antonio P\u00e9rez Ortiz (@japer3z) at the University of Alacant and also taught by professor Francisco de Borja Navarro Colorado (@bncolorado).</p> <p>For information regarding the course assessment, please refer to the course official info page. Some additional aspects not covered in the syllabus include:</p> <ul> <li>Practical assignments are to be completed individually or in pairs, as indicated in each assignment description. Each of the course blocks will have one or more practical assignments. The assignments in the first block contribute 30% to the final grade, the second block assignments contribute 50%, and the third block assignments contribute 20%.</li> <li>Attendance in practical sessions is mandatory. Roll call will be taken in each in-person session. A maximum of 1 unexcused absence is allowed. If a student accumulates more unexcused absences than the allowed limit, they will not be able to pass the practical component of the course in the C2 examination. However, they will have the opportunity to pass it in the C4.</li> </ul> <p>The source code of these pages, written in markdown for MkDocs, is available on GitHub.</p> <p>You can obtain a local copy of these pages (e.g., for offline access) by executing:</p> <pre><code>wget --mirror --no-parent --convert-links --page-requisites https://jaspock.github.io/tpln2324\n</code></pre> <p>Please note that the content may change throughout the course.</p> <p>The course has three blocks:</p> <ul> <li>Block 1: Introduction to computational linguistics and natural language processing</li> <li>Block 2: Architectures for written-text processing</li> <li>Block 3: Architectures for speech</li> </ul>"},{"location":"assignment-interpretability/","title":"Pr\u00e1ctica sobre interpretabilidad mecanicista de transformers","text":"<p>La interpretabilidad mecanicista en el contexto de la inteligencia artificial intenta dar una explicaci\u00f3n motivada del funcionamiento de los modelos de aprendizaje autom\u00e1tico. Es una propuesta muy importante de cara a generar confianza en los sistemas e inducir ciertos comportamientos en ellos. Dentro del campo de la interpretabilidad mecanicista existen un buen n\u00famero de t\u00e9cnicas que se pueden aplicar a los transformers. Aqu\u00ed nos centraremos en el parcheado de activaciones.</p> <p>El parcheado de activaciones interviene en una activaci\u00f3n espec\u00edfica de un modelo mediante la sustituci\u00f3n de una activaci\u00f3n corrompida con una activaci\u00f3n limpia. Se mide entonces c\u00f3mo afecta el cambio a la salida del modelo. Esto nos permite identificar qu\u00e9 activaciones son importantes para el resultado del modelo y localizar posibles causas de errores en la predicci\u00f3n. </p> <p>En nuestro caso particular, vas a escribir c\u00f3digo que ejecute la versi\u00f3n m\u00e1s peque\u00f1a de GPT2 (usa la cadena <code>gpt2</code> en el c\u00f3digo) con dos entradas diferentes: dos textos que solo se diferencien en un \u00fanico token. La idea es que al proporcionar al modelo la entrada corrompida, intervendremos en el embedding tras una cierta capa (uno solo cada vez) y lo parchearemos con el embedding correspondiente de la ejecuci\u00f3n limpia. Luego mediremos cu\u00e1nto cambia la predicci\u00f3n del siguiente token respecto a la ejecuci\u00f3n limpia. Si el cambio es significativo, entonces podemos estar seguros de que la activaci\u00f3n que hemos alterado es importante para la predicci\u00f3n. Este proceso de parcheado lo realizaremos para cada capa del modelo y para cada token de la entrada. Con toda esta informaci\u00f3n, obtendremos una gr\u00e1fica y sacaremos conclusiones. Por motivos que entender\u00e1s en un momento, los dos textos han de tener el mismo n\u00famero de tokens.</p>"},{"location":"assignment-interpretability/#ejemplo-de-analisis","title":"Ejemplo de an\u00e1lisis","text":"<p>Daremos un ejemplo para que se entienda mejor. Considera el siguiente texto de entrada: \"Michelle Jones was a top-notch student. Michelle\". Si se lo damos a GPT2 y estudiamos la probabilidad emitida por el modelo para el token que sigue a la segunda aparici\u00f3n de Michelle, obtendremos lo siguiente (solo se muestran los 20 tokens m\u00e1s probables): </p> Position Token index Token Probability 1 373 was 0.1634 2 5437 Jones 0.1396 3 338 's 0.0806 4 550 had 0.0491 5 318 is 0.0229 6 290 and 0.0227 7 11 , 0.0222 8 531 said 0.0134 9 468 has 0.0120 10 635 also 0.0117 11 1625 came 0.0091 12 1297 told 0.0084 13 1422 didn 0.0070 14 2993 knew 0.0067 15 1816 went 0.0061 16 561 would 0.0061 17 3111 worked 0.0055 18 750 did 0.0054 19 2486 Obama 0.0053 20 2492 wasn 0.0050 <p>Como era de esperar, el token \"Jones\" tiene una probabilidad notablemente elevada. Ahora, considera la entrada corrompida \"Michelle Smith was a top-notch student. Michelle\". Si le damos esta entrada a GPT2, esperamos que la probabilidad de \"Jones\" como continuaci\u00f3n del texto sea mucho menor que antes y que la de \"Smith\" sea mucho mayor, lo que (puedes comprobarlo) efectivamente ocurre. Pero queremos ir m\u00e1s all\u00e1 y saber qu\u00e9 embeddings son los que m\u00e1s influyen en esta diferencia. Dado que ambas entradas tienen 11 tokens (m\u00e1s adelante explicaremos c\u00f3mo averiguarlo) y que el transformer del modelo GPT2 peque\u00f1o tiene 12 capas, si nos centramos en los embeddings que se obtienen a la salida de cada capa, podemos parchear 11\u00d712 = 132 embeddings diferentes. Calcularemos, por tanto, 132 veces la diferencia entre el logit de \"Smith\" y el logit de \"Jones\" en la salida del \u00faltimo token de la entrada (\"Michelle\") en el modelo corrompido. Observa que tambi\u00e9n podr\u00edamos calcular las diferencias tras aplicar la funci\u00f3n softmax, pero en este caso no lo haremos.</p> <p>Una representaci\u00f3n en forma de mapa de calor del resultado es la siguiente:</p> <p></p> <p>Recuerda que en un gr\u00e1fico como el anterior, debido a la m\u00e1scara de atenci\u00f3n y a la disposici\u00f3n de las capas, la informaci\u00f3n fluye de izquierda a derecha y de arriba a abajo. Puedes ver c\u00f3mo intervenir en la primera columna no tiene efectos en la predicci\u00f3n del siguiente token, lo que tiene todo el sentido, ya que los embeddings que se parchean tienen exactamente los mismos valores en el modelo limpio y en el corrompido, ya que el contexto anterior es el mismo. Tampoco parece haber cambios al parchear los embeddings de la tercera a la antepen\u00faltima columna. Sin embargo, observa c\u00f3mo al intervenir los embeddings de muchas capas del segundo token, la predicci\u00f3n se decanta hacia \"Jones\" (el color se hace oscuro cuando la diferencia entre el logit de \"Smith\" y el de \"Jones\" se va haciendo negativa porque \"Jones\" tiene un logit mayor). Modificar los embeddings de las \u00faltimas capas del segundo token tiene efectos mucho menores, ya que el embedding apenas puede influir en el futuro de la secuencia. En la \u00faltima posici\u00f3n (\"Michelle\") se observa que los embeddings de las capas finales van anticip\u00e1ndose al token que tienen que predecir.</p> <p>Algunos textos corrompidos adicionales que puede ser interesante explorar son, por ejemplo, \"Jessica Jones was a top-notch student. Michelle\" o \"Michelle Smith was a top-notch student. Jessica\".</p>"},{"location":"assignment-interpretability/#entrega","title":"Entrega","text":"<p>En esta pr\u00e1ctica se trata de que programes el c\u00f3digo que te permite obtener gr\u00e1ficas y probabilidades como las anteriores, propongas tus propios textos limpios y corrompidos (intenta tirar de creatividad y no estudiar textos o fen\u00f3menos muy similares), realices un an\u00e1lisis parecido al anterior y escribas un informe dentro de un documento de entre 1500-2000 palabras (ambos l\u00edmites son estrictos) en el que presentes y comentes el c\u00f3digo que has implementado, adem\u00e1s de presentar tu enfoque, los resultados y las conclusiones pertinentes. Ser\u00e1n bienvenidas las ideas originales y los experimentos adicionales que se te ocurran. El documento en formato PDF ha de ser enviado por el sistema de tutor\u00eda de UACloud antes de las 23.55 del domingo 4 de febrero de 2024. La pr\u00e1ctica ha de ser realizada en parejas. Recuerda poner el nombre de ambos autores en el documento.</p>"},{"location":"assignment-interpretability/#codigo-base","title":"C\u00f3digo base","text":"<p>El c\u00f3digo base que usaremos es el de la implementaci\u00f3n de GPT2 que se encuentra en el repositorio minGPT de Andrej Karpathy. Su c\u00f3digo es el que ha inspirado nuestro c\u00f3digo del modelo transformers, por lo que no te resultar\u00e1 dif\u00edcil entenderlo. Puedes clonar el repositorio en tu ordenador o trabajar en un cuaderno de Google Colab como se indica m\u00e1s abajo.</p> <p>Debido a cambios en elementos externos, el c\u00f3digo actual no funciona tal cual. Para que funcione, tienes que cambiar la l\u00ednea 200 del fichero <code>mingpt/model.py</code> de:</p> <pre><code>assert len(keys) == len(sd)\n</code></pre> <p>a:</p> <pre><code>assert len(keys) == len([k for k in sd if not k.endswith(\".attn.bias\")])\n</code></pre>"},{"location":"assignment-interpretability/#tokenization","title":"Tokenization","text":"<p>El modelo GPT2 usa un tokenizador basado en BPE que trocea el texto de entrada en palabras o en unidades inferiores dependiendo de su frecuencia. El c\u00f3digo de minGPT permite descargar dicho tokenizador y usarlo para segmentar los textos. El siguiente c\u00f3digo muestra c\u00f3mo tokenizar un texto para obtener sus \u00edndices y viceversa.</p> <pre><code>from mingpt.bpe import BPETokenizer\n\ninput = \"Michelle Jones was a top-notch student. Michelle\"\nprint(\"Input:\", input)\nbpe = BPETokenizer()\n# bpe() gets a string and returns a 2D batch tensor \n# of indices with shape (1, input_length)\ntokens = bpe(input)[0]\nprint(\"Tokenized input:\", tokens)\ninput_length = tokens.shape[-1]\nprint(\"Number of input tokens:\", input_length)\n# bpe.decode gets a 1D tensor (list of indices) and returns a string\nprint(\"Detokenized input from indices:\", bpe.decode(tokens))  \ntokens_str = [bpe.decode(torch.tensor([token])) for token in tokens]\nprint(\"Detokenized input as strings: \" + '/'.join(tokens_str))\n</code></pre>"},{"location":"assignment-interpretability/#detalles-de-implementacion","title":"Detalles de implementaci\u00f3n","text":"<p>Lo siguiente son algunos detalles de implementaci\u00f3n que te pueden ser \u00fatiles, pero que no es necesario que sigas. </p> <p>Para conseguir un c\u00f3digo que te permita realizar el parcheado de activaciones te tendr\u00e1s que centrar en los ficheros <code>mingpt/model.py</code> y <code>generate.ipynb</code>. Si trabajas en local sin usar un notebook (recomendado) copia el c\u00f3digo de <code>generate.ipynb</code> en un fichero <code>generate.py</code> que puedas ejecutar desde la l\u00ednea de \u00f3rdenes.</p> <p>Tambi\u00e9n puedes trabajar directamente en una sesi\u00f3n de Google Colab. Aqu\u00ed tienes un proyecto (accede con tu cuenta de <code>gcloud.ua.es</code>) con instrucciones sobre c\u00f3mo usarlo para desarrollar. Sin embargo, es mucho m\u00e1s c\u00f3modo desarrollar en local (entre otras cosas, puedes trabajar con un mejor editor de texto que el de Colab y tambi\u00e9n depurar). Incluso si no tienes una GPU, el c\u00f3digo funciona sin problemas sobre CPU y solo tarda unos segundos m\u00e1s que sobre GPU al solo trabajar con un texto y con un modelo no excesivamente grande.</p> <p>A\u00f1ade a la funci\u00f3n <code>forward</code> del transformer c\u00f3digo que permita salvar (seg\u00fan el valor de cierto flag booleano recibido como par\u00e1metro) en una variable de instancia las activaciones de cada capa y cada posici\u00f3n. Recuerda hacer una copia profunda de los embeddings y no guardar \u00fanicamente una referencia que puede ser sobreescrita posteriormente; para ello, consulta la secuencia de llamadas <code>.detach().clone()</code> de PyTorch. A\u00f1ade tambi\u00e9n c\u00f3digo que permita (de nuevo en base a un par\u00e1metro booleano) parchear el embedding de una capa y posici\u00f3n concretas. </p> <p>A\u00f1ade tambi\u00e9n a la funci\u00f3n <code>forward</code> c\u00f3digo que guarde los logits del \u00faltimo token, que contienen la informaci\u00f3n que nos interesa sobre la predicci\u00f3n del siguiente token. Puedes guardar esta informaci\u00f3n en un atributo que luego puedes acceder desde el exterior de la clase. Observa que solo te interesa el vector correspondiente al \u00faltimo token.</p> <p>A\u00f1ade c\u00f3digo al fichero <code>generate.py</code> que divida el texto limpio en tokens, lo pase por el modelo a trav\u00e9s de la funci\u00f3n <code>generate</code> (pidi\u00e9ndole al modelo que guarde los embeddings intermedios) y muestre las continuaciones m\u00e1s probables a partir de los logits del \u00faltimo token. Ten en cuenta que si quieres saber la probabilidad de una continuaci\u00f3n como el token \"Jones\", por ejemplo, has de buscar el \u00edndice de dicho token en el vocabulario anteponi\u00e9ndole un espacio en blanco (<code>index = bpe(' Jones')</code>). Esto es as\u00ed porque el segmentador de BPE trata de forma diferente los tokens que aparecen al principio de la secuencia y los que aparecen en medio. Una vez tengas el \u00edndice del token, puedes acceder a la posici\u00f3n correspondiente del vector de logits y obtener la probabilidad no normalizada de que sea la continuaci\u00f3n.</p> <p>Despu\u00e9s, puedes trabajar con el texto corrupto. Incluye un doble bucle que itere sobre todas las capas y todas las posiciones y llame cada vez a <code>generate</code> pas\u00e1ndole la capa y la posici\u00f3n en la que realizar la intervenci\u00f3n. En cada paso, eval\u00faa la diferencia de logits oportuna y gu\u00e1rdala en una matriz de diferencias.</p> <p>Usa finalmente la funci\u00f3n <code>matshow</code> de <code>matplotlib</code> para visualizar la matriz de diferencias.</p>"},{"location":"assignment-interpretability/#una-explicacion-mas-informal","title":"Una explicaci\u00f3n m\u00e1s informal","text":"<p>La siguiente explicaci\u00f3n informal puede que te ayude a entender mejor el objetivo de la pr\u00e1ctica.</p> <p>Considera para simplificar la frase \"a b c\" y la versi\u00f3n corrompida \"d e f\". En general, habr\u00e1 muchos m\u00e1s tokens en com\u00fan, pero as\u00ed queda todo m\u00e1s claro en la siguiente discusi\u00f3n. Considera que el modelo neuronal basado en el transformer tiene 5 capas de atenci\u00f3n. Considera que vamos a estudiar qu\u00e9 embeddings son importantes para la predicci\u00f3n de que tras estas frases vaya el token \"X\".</p> <p>Se trata primero de que permitas que en la funci\u00f3n forward del transformer (clase <code>GPT</code>) se puedan guardar (por ejemplo en una lista de listas de tensores) los 3x5=15 embeddings que se generan a la salida de cada una de las capas cuando se procesa la frase \"a b c\". En el enunciado se dan algunos detalles porque no puedes guardar simplemente la referencia a los tensores, ya que se modificar\u00e1n la pr\u00f3xima vez que llames a forward, sino que has de clonar los tensores (lo que se llama \"copia defensiva\"). Con esto tendr\u00e1s almacenados los 15 tensores (embeddings) de la frase limpia.</p> <p>Gu\u00e1rdate tambi\u00e9n los logits tras la \u00faltima capa. En particular, solo necesitar\u00e1s los de la \u00faltima posici\u00f3n (es decir, los logits correspondientes al token \"c\"), que te dan una medida de la probabilidad del siguiente token, es decir, del token que ir\u00e1 tras \"c\". Recuerda que estos logits no son realmente probabilidades (son valores como -11.1, -0.5, 0.78, o 2.32323) porque no se les ha aplicado la funci\u00f3n softmax, pero trabajar con ellos es m\u00e1s c\u00f3modo que trabajar con las probabilidades porque tenemos valores con un rango m\u00e1s amplio. No obstante, el estudio podr\u00eda hacerse igualmente con probabilidades estrictas. En realidad, ni siquiera necesitas guardarte todos los logits, sino solo el escalar que corresponde al token \"X\" porque es lo \u00fanico que usar\u00e1s despu\u00e9s.</p> <p>Ahora le das al modelo la versi\u00f3n corrompida \"d e f\", indic\u00e1ndole que no sobreescriba la copia de los embeddings que obtuvimos con la frase limpia. La frase corrompida ha de tener el mismo n\u00famero de tokens que la limpia para que la siguiente discusi\u00f3n tenga sentido. La idea es modificar uno solo de los 15 embeddings que se producen mientras se procesa la frase sucia. Si, por ejemplo, nos centramos en el embedding del primer token (\"d\") tras la primera capa, se tratar\u00eda de que el c\u00f3digo de la funci\u00f3n forward opere \"casi\" de la forma normal, pero cuando se obtenga la salida de la primera capa y antes de pasarla como entrada a la segunda capa, se ha de modificar el embedding correspondiente a la primera palabra (solo ese) y sustituirlo por el embedding correspondiente (de la misma capa y posici\u00f3n) que te guardaste para la frase limpia (es decir, en este caso, ser\u00eda el embedding que te guardaste tras la primera capa para el token \"a\"). Con esto, la segunda capa recibir\u00e1 como entrada el embedding que se gener\u00f3 para \"a\" en lugar del de \"d\".</p> <p>Tras intervenir en el embedding de la posici\u00f3n 1 tras la capa 1, el resto del modelo trabaja sin ning\u00fan \"contratiempo\". De la misma manera que antes, ahora miramos los logits de la predicci\u00f3n del token que va tras el \u00faltimo token de la frase corrompida (es decir, \"f\"). Y nos centramos en el valor del logit de la predicci\u00f3n del token \"X\". La diferencia entre este valor y el que nos guardamos para la frase limpia nos da una idea de c\u00f3mo de relevante es el embedding de la capa 1 y posici\u00f3n 1 en la predicci\u00f3n del token \"X\". En el enunciado se muestra c\u00f3mo algunos embeddings son mucho m\u00e1s relevantes que otros. Y t\u00fa tienes que hacer un estudio similar con diferentes frases.</p> <p>Si repites la operaci\u00f3n anterior con los otros 14 embeddings (llamando 14 veces m\u00e1s a la funci\u00f3n forward), terminar\u00e1s teniendo 15 diferencias de logits (15 valores escalares) que puedes representar en un mapa de calor de 3x5 como se ve m\u00e1s arriba.</p> <p>Finalmente, ten en cuenta que la discusi\u00f3n de este apartado tiene una peque\u00f1a simplificaci\u00f3n respecto a lo que se pide en el enunciado de m\u00e1s arriba. All\u00ed se propon\u00eda calcular la diferencia entre el logit de \"Smith\" y el logit de \"Jones\" en la salida del \u00faltimo token en el modelo corrompido, lo que da un poco m\u00e1s de informaci\u00f3n que la diferencia que hemos explicado en este apartado, es decir, la diferencia entre la predicci\u00f3n de un solo token (\"Jones\") en la frase limpia y la corrompida, en lugar de dos tokens en la frase corrompida. En realidad, cualquiera de las dos opciones es v\u00e1lida para llegar a las conclusiones que nos interesan: que en la frase corrompida, el logit de \"Jones\" se hace mucho menor excepto para ciertas intervenciones. Si quieres que tu mapa de calor coincida con el de este enunciado, sigue el enfoque basado en los dos tokens \"Jones\" y \"Smith\".</p>"},{"location":"assignment-interpretability/#ampliar-conocimientos","title":"Ampliar conocimientos","text":"<p>Lo anterior es solo uno de los m\u00faltiples an\u00e1lisis que se han propuesto dentro de la interpretabilidad mecanicista. Para esta pr\u00e1ctica no se espera que vayas m\u00e1s all\u00e1 de esto, pero si te interesa conocer un par de an\u00e1lisis m\u00e1s puedes consultar este tutorial. Observa que aunque el tutorial usa una librer\u00eda para parchear las activaciones, en esta pr\u00e1ctica no puedes usar ninguna librer\u00eda para ello y lo has de hacer directamente sobre el c\u00f3digo de minGPT. Una revisi\u00f3n mucho m\u00e1s detallada sobre la interpretabilidad mecanicista se puede encontrar en este trabajo de Neel Nanda.</p>"},{"location":"cl/","title":"Introduction to computational linguistics and natural language processing","text":"<p>The content of this section is here: a PDF with the introduction to NLP (in Spanish).</p> <p>It covers the next topics:</p> <ul> <li>Introduction to Natural Language Processing</li> <li>Symbolic models for Part of Speech tagging </li> <li>Symbolic models for syntactic parsing</li> <li>Symbolic models for semantic analysis</li> </ul> <p>NEW CONTENT!! Chapters 6 and 7 are here. This PDF covers two new topics:</p> <ul> <li>vector space models of semantics and</li> <li>some open issues in NLP.</li> </ul> <p>After reading them (1 hour aprox.), you must:</p> <ul> <li>answer Test 2 (below), and</li> <li>read the article \"The Future of Computational Linguistics: On Beyond Alchemy\" here (Reading time: one hour and a half).</li> </ul>"},{"location":"cl/#test-1","title":"Test 1","text":"<p>Find here the first test. It must be answered before the first face-to-face class (deadline: 23:59 CET, Dec 12 2023)</p>"},{"location":"cl/#practica-1","title":"Pr\u00e1ctica 1","text":"<p>Here is the description for Assignment 1, which will be done in the class on 12/13/2023.</p> <p>Aqu\u00ed ten\u00e9is el enunciado de la Pr\u00e1ctica 1, que ser\u00e1 realizado en la clase del 13/12/2023.</p>"},{"location":"cl/#test-2","title":"Test 2","text":"<p>Find here the second test. It must be answered before the second face-to-face class (deadline: 23:59 CET, Dec 19 2023)</p>"},{"location":"speech/","title":"Architectures for speech","text":"<p>En este bloque se aborda brevemente el estudio de algunos modelos neuronales utilizados para procesar voz. El profesor de este bloque es Juan Antonio P\u00e9rez Ortiz. </p> <p>Los materiales de clase complementan la lectura de algunos cap\u00edtulos de un libro de texto (\"Speech and Language Processing\" de Dan Jurafsky y James H. Martin, borrador de la tercera edici\u00f3n, disponible online) con anotaciones realizadas por el profesor.</p>"},{"location":"speech/#primera-sesion-de-este-bloque-19-de-enero-de-2024","title":"Primera sesi\u00f3n de este bloque (19 de enero de 2024)","text":""},{"location":"speech/#contenidos-a-preparar-antes-de-la-sesion-del-19012024","title":"Contenidos a preparar antes de la sesi\u00f3n del 19/01/2024","text":"<p>Las actividades a realizar antes de esta clase son:</p> <ul> <li>Lectura y estudio de los contenidos de esta p\u00e1gina sobre reconocimiento de voz. Como ver\u00e1s, la p\u00e1gina te indica qu\u00e9 contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo prop\u00f3sito es ayudarte a entender los conceptos clave del cap\u00edtulo. Despu\u00e9s, realiza una segunda lectura del cap\u00edtulo del libro. Tras acabar con esta parte, lee la descripci\u00f3n de arquitecturas modernas concretas para el reconocimiento de voz. En total, esta parte deber\u00eda llevarte unas 4 horas \ud83d\udd52\ufe0f de trabajo.</li> <li>Despu\u00e9s, realiza este test de evaluaci\u00f3n de estos contenidos. Son pocas preguntas (menos que en tests anteriores, de hecho) y te llevar\u00e1 unos minutos.</li> </ul>"},{"location":"speech/#contenidos-para-la-sesion-presencial-del-19012024","title":"Contenidos para la sesi\u00f3n presencial del 19/01/2024","text":"<p>En la clase presencial (2,5 horas \ud83d\udd52\ufe0f de duraci\u00f3n), veremos c\u00f3mo se implementa un sistema de clasificaci\u00f3n de voz en PyTorch. Para ello, utilizaremos la librer\u00eda <code>torchaudio</code>, que es parte de PyTorch. En particular, miraremos por encima esta gu\u00eda de manipulaci\u00f3n de audio con torchaudio (\u00fanicamente la representaci\u00f3n de ondas y la obtenci\u00f3n del espectrograma) y la implementaci\u00f3n del clasificador de voz; ambos documentos tienen un enlace al comienzo a sendos cuadernos de Google Colab. Los dos tutoriales los veremos este curso solo en clase con el prop\u00f3sito de complementar los contenidos te\u00f3ricos, pero no has de estudiarlos para el examen.</p>"},{"location":"text/","title":"Architectures for written-text processing","text":"<p>En este bloque se aborda el estudio de algunos modelos neuronales utilizados para procesar textos. El profesor de este bloque es Juan Antonio P\u00e9rez Ortiz. El bloque comienza con un repaso del funcionamiento del regresor log\u00edstico, que nos servir\u00e1 para asentar los conocimientos necesarios para entender posteriores modelos. A continuaci\u00f3n se estudia con cierto nivel de detalle skip-grams, uno de los algoritmos para la obtenci\u00f3n de embeddings incontextuales de palabras. Despu\u00e9s se repasa el funcionamiento de las arquitecturas neuronales feedforward y se estudia su aplicaci\u00f3n a modelos de lengua. El objetivo \u00faltimo es abordar el estudio de la arquitectura m\u00e1s importante de los sistemas actuales de procesamiento de textos: el transformer. Una vez estudiadas estas arquitecturas, finalizaremos con un an\u00e1lisis del funcionamiento de los modelos preentrenados (modelos fundacionales), en general, y de los modelos de lengua, en particular.</p> <p>Los materiales de clase complementan la lectura de algunos cap\u00edtulos de un libro de texto (\"Speech and Language Processing\" de Dan Jurafsky y James H. Martin, borrador de la tercera edici\u00f3n, disponible online) con anotaciones realizadas por el profesor.</p>"},{"location":"text/#primera-sesion-de-este-bloque-20-de-diciembre-de-2023","title":"Primera sesi\u00f3n de este bloque (20 de diciembre de 2023)","text":""},{"location":"text/#contenidos-a-preparar-antes-de-la-sesion-del-20122023","title":"Contenidos a preparar antes de la sesi\u00f3n del 20/12/2023","text":"<p>Las actividades a realizar antes de esta clase son:</p> <ul> <li>Lectura y estudio de los contenidos de esta p\u00e1gina sobre regresi\u00f3n log\u00edstica. Como ver\u00e1s, la p\u00e1gina te indica qu\u00e9 contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo prop\u00f3sito es ayudarte a entender los conceptos clave del cap\u00edtulo. Despu\u00e9s, realiza una segunda lectura del cap\u00edtulo del libro. En total, esta parte deber\u00eda llevarte unas 3 horas \ud83d\udd52\ufe0f de trabajo.</li> <li>Visionado y estudio de los tutoriales en v\u00eddeo de esta playlist oficial de PyTorch.  Estudia al menos los 4 primeros v\u00eddeos (\u201cIntroduction to PyTorch\u201d, \u201cIntroduction to PyTorch Tensors\u201d, \u201cThe Fundamentals of Autograd\u201d y \u201cBuilding Models with PyTorch\u201d). En total, esta parte deber\u00eda llevarte unas 2 horas \ud83d\udd52\ufe0f de trabajo.</li> <li>Tras acabar con las dos partes anteriores, realiza este test de evaluaci\u00f3n de estos contenidos. Son pocas preguntas y te llevar\u00e1 unos minutos.</li> </ul>"},{"location":"text/#contenidos-para-la-sesion-presencial-del-20122023","title":"Contenidos para la sesi\u00f3n presencial del 20/12/2023","text":"<p>En la clase presencial (2,5 horas \ud83d\udd52\ufe0f de duraci\u00f3n), veremos c\u00f3mo se implementa un regresor log\u00edstico en PyTorch siguiendo las implementaciones de un regresor log\u00edstico binario  y de uno multinomial  que se comentan en este apartado.</p> <p>La idea es que vayas estudiando y modificando ligeramente los notebooks que vayamos estudiando. En una clase posterior se presentar\u00e1 una pr\u00e1ctica m\u00e1s avanzada que implicar\u00e1 modificar el c\u00f3digo del transformer.</p>"},{"location":"text/#segunda-sesion-10-de-enero-de-2024","title":"Segunda sesi\u00f3n (10 de enero de 2024)","text":""},{"location":"text/#contenidos-a-preparar-antes-de-la-sesion-del-10012024","title":"Contenidos a preparar antes de la sesi\u00f3n del 10/01/2024","text":"<p>Las actividades a realizar antes de esta clase son:</p> <ul> <li>Lectura y estudio de los contenidos de esta p\u00e1gina sobre los embeddings. Como ver\u00e1s, la p\u00e1gina te indica qu\u00e9 contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo prop\u00f3sito es ayudarte a entender los conceptos clave del cap\u00edtulo. Despu\u00e9s, realiza una segunda lectura del cap\u00edtulo del libro. En total, esta parte deber\u00eda llevarte unas 4 horas \ud83d\udd52\ufe0f de trabajo.</li> <li>Lectura y estudio de los contenidos de esta p\u00e1gina sobre las redes neuronales hacia delante y su uso como modelos de lengua muy b\u00e1sicos. Realiza al menos dos lecturas complementadas con las notas del profesor como en el punto anterior. En total, esta parte deber\u00eda llevarte unas 2 horas \ud83d\udd52\ufe0f de trabajo.</li> <li>Lectura y estudio de los contenidos de esta p\u00e1gina de introducci\u00f3n a los transformers. Realiza, como siempre, al menos dos lecturas complementadas con las notas del profesor. En total, esta parte deber\u00eda llevarte unas 4 horas \ud83d\udd52\ufe0f de trabajo.</li> <li>Tras acabar con las partes anteriores, realiza este test de evaluaci\u00f3n de estos contenidos. Son pocas preguntas y te llevar\u00e1 unos minutos.</li> <li>Aprovecha, si te queda tiempo, para repasar los contenidos de la primera sesi\u00f3n.</li> </ul>"},{"location":"text/#contenidos-para-la-sesion-presencial-del-10012024","title":"Contenidos para la sesi\u00f3n presencial del 10/01/2024","text":"<p>En la clase presencial (5 horas \ud83d\udd52\ufe0f de duraci\u00f3n), veremos c\u00f3mo se implementa en PyTorch el algoritmo de skip-grams  , un modelo de lengua basado en red neuronal hacia delante   y un transformer   siguiendo las implementaciones que se comentan en este apartado y los dos siguientes.</p> <p>La idea es que vayas estudiando y modificando ligeramente los notebooks que vayamos estudiando. En una clase posterior se presentar\u00e1 una pr\u00e1ctica m\u00e1s avanzada que implicar\u00e1 modificar el c\u00f3digo del transformer.</p>"},{"location":"text/#tercera-sesion-17-de-enero-de-2024","title":"Tercera sesi\u00f3n (17 de enero de 2024)","text":""},{"location":"text/#contenidos-a-preparar-antes-de-la-sesion-del-17012024","title":"Contenidos a preparar antes de la sesi\u00f3n del 17/01/2024","text":"<p>Las actividades a realizar antes de esta clase son:</p> <ul> <li>Lectura y estudio de los contenidos de esta p\u00e1gina sobre, por un lado, el modelo transformer completo (con codificador y descodificador) y, por otro, los posibles usos de una arquitectura que solo incluye el codificador. Como ver\u00e1s, la p\u00e1gina te indica qu\u00e9 contenidos has de leer del libro. En particular, tendr\u00e1s que leer algunas secciones del cap\u00edtulo sobre traducci\u00f3n autom\u00e1tica y otras del cap\u00edtulo sobre modelos preentrenados, adem\u00e1s de alguna secci\u00f3n suelta sobre beam search y tokenizaci\u00f3n en subpalabras. Tras una primera lectura, lee las anotaciones del profesor, cuyo prop\u00f3sito es ayudarte a entender los conceptos clave de cada apartado. Despu\u00e9s, realiza una segunda lectura de los contenidos del libro. En total, esta parte deber\u00eda llevarte unas 4 horas \ud83d\udd52\ufe0f de trabajo.</li> <li>Visonado y estudio de la clase de Jesse Mu titulada \"Prompting, Reinforcement Learning from Human Feedback\" del curso CS224N de Stanford de 2023 sobre modelos de lengua basados en el descodificador del transformer. En total, esta parte deber\u00eda llevarte unas 2 horas \ud83d\udd52\ufe0f de trabajo, porque tendr\u00e1s que tomar notas del v\u00eddeo para no tener que verlo cada vez que quieras repasar algo; para tomar notas, te puede venir bien descargar las diapositivas y escribir sobre ellas. Puedes quedarte solo con las ideas b\u00e1sicas de lo que se comenta entre los minutos 39 y 46, porque las ecuaciones del aprendizaje por refuerzo son un tema no prioritario para esta asignatura que ver\u00e1s en otras asignaturas. Es importante que antes de ver el v\u00eddeo repases lo que ya estudiaste sobre los transformers como modelo de lengua basado en el descodificador. Que no te confunda que a los modelos basados en codificador tambi\u00e9n se les conozca a veces con el nombre de modelos de lengua. En este v\u00eddeo se habla de las propiedades de modelos basados en descodificador que han sido entrenados para predecir el siguiente token de una secuencia.</li> <li>Estudia la descripci\u00f3n sobre modelos multiling\u00fces que se hace en este apartado de una de las p\u00e1ginas sobre transformers. Es un apartado breve que te llevar\u00e1 unos \ud83d\udd52\ufe0f 15 minutos.</li> <li>Tras acabar con las partes anteriores, realiza este test de evaluaci\u00f3n de estos contenidos. Son pocas preguntas y te llevar\u00e1 unos minutos.</li> <li>Aprovecha, si te queda tiempo, para repasar todos los contenidos de las sesiones anteriores.</li> </ul>"},{"location":"text/#contenidos-para-la-sesion-presencial-del-17012024","title":"Contenidos para la sesi\u00f3n presencial del 17/01/2024","text":"<p>En la clase presencial (5 horas \ud83d\udd52\ufe0f de duraci\u00f3n), veremos c\u00f3mo se implementa sobre nuestro c\u00f3digo de la arquitectura transformer tanto un modelo de lengua basado en descodificador  como un modelo de reconocimiento de entidades nombradas  basado en codificador. </p> <p>Aprovecharemos para repasar algunos aspectos del c\u00f3digo de sesiones anteriores y relacionar los aspectos te\u00f3ricos con los pr\u00e1cticos. Presentaremos tambi\u00e9n la pr\u00e1ctica que tienes que entregar para este bloque de la asignatura.</p>"},{"location":"text/#cuarta-sesion-19-de-enero-de-2024","title":"Cuarta sesi\u00f3n (19 de enero de 2024)","text":"<p>Esta cuarta sesi\u00f3n es realmente la primera y \u00fanica sesi\u00f3n del tema de voz. Mira la p\u00e1gina sobre voz para ver los contenidos previos a esta sesi\u00f3n.</p>"}]}