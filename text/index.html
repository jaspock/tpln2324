
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../speech/">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.4">
    
    
      
        <title>Architectures for written-text processing - Técnicas de Procesamiento del Lenguaje Natural</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.50c56a3b.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#architectures-for-written-text-processing" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Técnicas de Procesamiento del Lenguaje Natural" class="md-header__button md-logo" aria-label="Técnicas de Procesamiento del Lenguaje Natural" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Técnicas de Procesamiento del Lenguaje Natural
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Architectures for written-text processing
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Técnicas de Procesamiento del Lenguaje Natural" class="md-nav__button md-logo" aria-label="Técnicas de Procesamiento del Lenguaje Natural" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Técnicas de Procesamiento del Lenguaje Natural
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Técnicas de Procesamiento del Lenguaje Natural, 2023-2024
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../assignment-interpretability/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Práctica sobre interpretabilidad mecanicista de transformers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../cl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to computational linguistics and natural language processing
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Architectures for speech
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Architectures for written-text processing
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Architectures for written-text processing
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#primera-sesion-de-este-bloque-20-de-diciembre-de-2023" class="md-nav__link">
    <span class="md-ellipsis">
      Primera sesión de este bloque (20 de diciembre de 2023)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Primera sesión de este bloque (20 de diciembre de 2023)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contenidos-a-preparar-antes-de-la-sesion-del-20122023" class="md-nav__link">
    <span class="md-ellipsis">
      Contenidos a preparar antes de la sesión del 20/12/2023
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contenidos-para-la-sesion-presencial-del-20122023" class="md-nav__link">
    <span class="md-ellipsis">
      Contenidos para la sesión presencial del 20/12/2023
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#segunda-sesion-10-de-enero-de-2024" class="md-nav__link">
    <span class="md-ellipsis">
      Segunda sesión (10 de enero de 2024)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Segunda sesión (10 de enero de 2024)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contenidos-a-preparar-antes-de-la-sesion-del-10012024" class="md-nav__link">
    <span class="md-ellipsis">
      Contenidos a preparar antes de la sesión del 10/01/2024
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contenidos-para-la-sesion-presencial-del-10012024" class="md-nav__link">
    <span class="md-ellipsis">
      Contenidos para la sesión presencial del 10/01/2024
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tercera-sesion-17-de-enero-de-2024" class="md-nav__link">
    <span class="md-ellipsis">
      Tercera sesión (17 de enero de 2024)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Tercera sesión (17 de enero de 2024)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contenidos-a-preparar-antes-de-la-sesion-del-17012024" class="md-nav__link">
    <span class="md-ellipsis">
      Contenidos a preparar antes de la sesión del 17/01/2024
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contenidos-para-la-sesion-presencial-del-17012024" class="md-nav__link">
    <span class="md-ellipsis">
      Contenidos para la sesión presencial del 17/01/2024
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cuarta-sesion-19-de-enero-de-2024" class="md-nav__link">
    <span class="md-ellipsis">
      Cuarta sesión (19 de enero de 2024)
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#primera-sesion-de-este-bloque-20-de-diciembre-de-2023" class="md-nav__link">
    <span class="md-ellipsis">
      Primera sesión de este bloque (20 de diciembre de 2023)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Primera sesión de este bloque (20 de diciembre de 2023)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contenidos-a-preparar-antes-de-la-sesion-del-20122023" class="md-nav__link">
    <span class="md-ellipsis">
      Contenidos a preparar antes de la sesión del 20/12/2023
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contenidos-para-la-sesion-presencial-del-20122023" class="md-nav__link">
    <span class="md-ellipsis">
      Contenidos para la sesión presencial del 20/12/2023
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#segunda-sesion-10-de-enero-de-2024" class="md-nav__link">
    <span class="md-ellipsis">
      Segunda sesión (10 de enero de 2024)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Segunda sesión (10 de enero de 2024)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contenidos-a-preparar-antes-de-la-sesion-del-10012024" class="md-nav__link">
    <span class="md-ellipsis">
      Contenidos a preparar antes de la sesión del 10/01/2024
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contenidos-para-la-sesion-presencial-del-10012024" class="md-nav__link">
    <span class="md-ellipsis">
      Contenidos para la sesión presencial del 10/01/2024
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tercera-sesion-17-de-enero-de-2024" class="md-nav__link">
    <span class="md-ellipsis">
      Tercera sesión (17 de enero de 2024)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Tercera sesión (17 de enero de 2024)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contenidos-a-preparar-antes-de-la-sesion-del-17012024" class="md-nav__link">
    <span class="md-ellipsis">
      Contenidos a preparar antes de la sesión del 17/01/2024
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contenidos-para-la-sesion-presencial-del-17012024" class="md-nav__link">
    <span class="md-ellipsis">
      Contenidos para la sesión presencial del 17/01/2024
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cuarta-sesion-19-de-enero-de-2024" class="md-nav__link">
    <span class="md-ellipsis">
      Cuarta sesión (19 de enero de 2024)
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="architectures-for-written-text-processing">Architectures for written-text processing<a class="headerlink" href="#architectures-for-written-text-processing" title="Permanent link">&para;</a></h1>
<p>En este bloque se aborda el estudio de algunos modelos neuronales utilizados para procesar textos. El profesor de este bloque es Juan Antonio Pérez Ortiz. El bloque comienza con un repaso del funcionamiento del regresor logístico, que nos servirá para asentar los conocimientos necesarios para entender posteriores modelos. A continuación se estudia con cierto nivel de detalle <em>skip-grams</em>, uno de los algoritmos para la obtención de <em>embeddings</em> incontextuales de palabras. Después se repasa el funcionamiento de las arquitecturas neuronales <em>feedforward</em> y se estudia su aplicación a modelos de lengua. El objetivo último es abordar el estudio de la arquitectura más importante de los sistemas actuales de procesamiento de textos: el transformer. Una vez estudiadas estas arquitecturas, finalizaremos con un análisis del funcionamiento de los modelos preentrenados (modelos fundacionales), en general, y de los modelos de lengua, en particular.</p>
<p>Los materiales de clase complementan la lectura de algunos capítulos de un libro de texto ("Speech and Language Processing" de Dan Jurafsky y James H. Martin, borrador de la tercera edición, disponible online) con anotaciones realizadas por el profesor.</p>
<h2 id="primera-sesion-de-este-bloque-20-de-diciembre-de-2023">Primera sesión de este bloque (20 de diciembre de 2023)<a class="headerlink" href="#primera-sesion-de-este-bloque-20-de-diciembre-de-2023" title="Permanent link">&para;</a></h2>
<h3 id="contenidos-a-preparar-antes-de-la-sesion-del-20122023">Contenidos a preparar antes de la sesión del 20/12/2023<a class="headerlink" href="#contenidos-a-preparar-antes-de-la-sesion-del-20122023" title="Permanent link">&para;</a></h3>
<p>Las actividades a realizar antes de esta clase son:</p>
<ul>
<li>Lectura y estudio de los contenidos de <a href="https://dlsi.ua.es/~japerez/materials/transformers/regresor/">esta página</a> sobre regresión logística. Como verás, la página te indica qué contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo propósito es ayudarte a entender los conceptos clave del capítulo. Después, realiza una segunda lectura del capítulo del libro. En total, esta parte debería llevarte unas 3 horas 🕒️ de trabajo.</li>
<li>Visionado y estudio de los tutoriales en vídeo de esta <a href="https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN">playlist oficial de PyTorch</a>.  Estudia al menos los 4 primeros vídeos (“Introduction to PyTorch”, “Introduction to PyTorch Tensors”, “The Fundamentals of Autograd” y “Building Models with PyTorch”). En total, esta parte debería llevarte unas 2 horas 🕒️ de trabajo.</li>
<li>Tras acabar con las dos partes anteriores, realiza este <a href="https://forms.gle/V3U9MTHo7c9DNhkc6">test de evaluación</a> de estos contenidos. Son pocas preguntas y te llevará unos minutos.</li>
</ul>
<h3 id="contenidos-para-la-sesion-presencial-del-20122023">Contenidos para la sesión presencial del 20/12/2023<a class="headerlink" href="#contenidos-para-la-sesion-presencial-del-20122023" title="Permanent link">&para;</a></h3>
<p>En la clase presencial (2,5 horas 🕒️ de duración), veremos cómo se implementa un regresor logístico en PyTorch siguiendo las implementaciones de un regresor logístico binario <a href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/logistic.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab\"></a> y de uno multinomial <a href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/softmax.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab\"></a> que se comentan en <a href="https://dlsi.ua.es/~japerez/materials/transformers/implementacion/#codigo-para-un-regresor-logistico-y-uno-multinomial">este apartado</a>.</p>
<p>La idea es que vayas estudiando y modificando ligeramente los notebooks que vayamos estudiando. En una clase posterior se presentará una práctica más avanzada que implicará modificar el código del transformer.</p>
<h2 id="segunda-sesion-10-de-enero-de-2024">Segunda sesión (10 de enero de 2024)<a class="headerlink" href="#segunda-sesion-10-de-enero-de-2024" title="Permanent link">&para;</a></h2>
<h3 id="contenidos-a-preparar-antes-de-la-sesion-del-10012024">Contenidos a preparar antes de la sesión del 10/01/2024<a class="headerlink" href="#contenidos-a-preparar-antes-de-la-sesion-del-10012024" title="Permanent link">&para;</a></h3>
<p>Las actividades a realizar antes de esta clase son:</p>
<ul>
<li>Lectura y estudio de los contenidos de <a href="https://dlsi.ua.es/~japerez/materials/transformers/embeddings/">esta página</a> sobre los embeddings. Como verás, la página te indica qué contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo propósito es ayudarte a entender los conceptos clave del capítulo. Después, realiza una segunda lectura del capítulo del libro. En total, esta parte debería llevarte unas 4 horas 🕒️ de trabajo.</li>
<li>Lectura y estudio de los contenidos de <a href="https://dlsi.ua.es/~japerez/materials/transformers/ffw/">esta página</a> sobre las redes neuronales hacia delante y su uso como modelos de lengua muy básicos. Realiza al menos dos lecturas complementadas con las notas del profesor como en el punto anterior. En total, esta parte debería llevarte unas 2 horas 🕒️ de trabajo.</li>
<li>Lectura y estudio de los contenidos de <a href="https://dlsi.ua.es/~japerez/materials/transformers/attention/">esta página</a> de introducción a los transformers. Realiza, como siempre, al menos dos lecturas complementadas con las notas del profesor. En total, esta parte debería llevarte unas 4 horas 🕒️ de trabajo.</li>
<li>Tras acabar con las partes anteriores, realiza este <a href="https://forms.gle/7KDwRtXcrpxsKjHp7">test de evaluación</a> de estos contenidos. Son pocas preguntas y te llevará unos minutos.</li>
<li>Aprovecha, si te queda tiempo, para repasar los contenidos de la primera sesión.</li>
</ul>
<h3 id="contenidos-para-la-sesion-presencial-del-10012024">Contenidos para la sesión presencial del 10/01/2024<a class="headerlink" href="#contenidos-para-la-sesion-presencial-del-10012024" title="Permanent link">&para;</a></h3>
<p>En la clase presencial (5 horas 🕒️ de duración), veremos cómo se implementa en PyTorch el algoritmo de skip-grams <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/skipgram.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>, un modelo de lengua basado en red neuronal hacia delante <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/ffnn.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> y un transformer <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/transformer.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> siguiendo las implementaciones que se comentan en <a href="https://www.dlsi.ua.es/~japerez/materials/transformers/implementacion/#codigo-para-skip-grams">este apartado</a> y los dos siguientes.</p>
<p>La idea es que vayas estudiando y modificando ligeramente los notebooks que vayamos estudiando. En una clase posterior se presentará una práctica más avanzada que implicará modificar el código del transformer.</p>
<h2 id="tercera-sesion-17-de-enero-de-2024">Tercera sesión (17 de enero de 2024)<a class="headerlink" href="#tercera-sesion-17-de-enero-de-2024" title="Permanent link">&para;</a></h2>
<h3 id="contenidos-a-preparar-antes-de-la-sesion-del-17012024">Contenidos a preparar antes de la sesión del 17/01/2024<a class="headerlink" href="#contenidos-a-preparar-antes-de-la-sesion-del-17012024" title="Permanent link">&para;</a></h3>
<p>Las actividades a realizar antes de esta clase son:</p>
<ul>
<li>Lectura y estudio de los contenidos de <a href="https://dlsi.ua.es/~japerez/materials/transformers/attention2/">esta página</a> sobre, por un lado, el modelo transformer completo (con codificador y descodificador) y, por otro, los posibles usos de una arquitectura que solo incluye el codificador. Como verás, la página te indica qué contenidos has de leer del libro. En particular, tendrás que leer algunas secciones del capítulo sobre traducción automática y otras del capítulo sobre modelos preentrenados, además de alguna sección suelta sobre <em>beam search</em> y tokenización en subpalabras. Tras una primera lectura, lee las anotaciones del profesor, cuyo propósito es ayudarte a entender los conceptos clave de cada apartado. Después, realiza una segunda lectura de los contenidos del libro. En total, esta parte debería llevarte unas 4 horas 🕒️ de trabajo.</li>
<li>Visonado y estudio de la clase de Jesse Mu titulada "<a href="https://youtu.be/SXpJ9EmG3s4?si=j4B1U2Z-JCyYJwlc">Prompting, Reinforcement Learning from Human Feedback</a>" del curso CS224N de Stanford de 2023 sobre modelos de lengua basados en el descodificador del transformer. En total, esta parte debería llevarte unas 2 horas 🕒️ de trabajo, porque tendrás que tomar notas del vídeo para no tener que verlo cada vez que quieras repasar algo; para tomar notas, te puede venir bien descargar las <a href="https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf">diapositivas</a> y escribir sobre ellas. Puedes quedarte solo con las ideas básicas de lo que se comenta entre los minutos 39 y 46, porque las ecuaciones del aprendizaje por refuerzo son un tema no prioritario para esta asignatura que verás en otras asignaturas. Es importante que antes de ver el vídeo repases lo que ya estudiaste sobre los <a href="https://dlsi.ua.es/~japerez/materials/transformers/attention/">transformers</a> como modelo de lengua basado en el descodificador. Que no te confunda que a los modelos basados en codificador también se les conozca a veces con el nombre de modelos de lengua. En este vídeo se habla de las propiedades de modelos basados en descodificador que han sido entrenados para predecir el siguiente token de una secuencia.</li>
<li>Estudia la descripción sobre <a href="https://dlsi.ua.es/~japerez/materials/transformers/attention2/#multilingual-models">modelos multilingües</a> que se hace en este apartado de una de las páginas sobre transformers. Es un apartado breve que te llevará unos 🕒️ 15 minutos.</li>
<li>Tras acabar con las partes anteriores, realiza este <a href="https://forms.gle/GRK5SLc3STkup8at9">test de evaluación</a> de estos contenidos. Son pocas preguntas y te llevará unos minutos.</li>
<li>Aprovecha, si te queda tiempo, para repasar todos los contenidos de las sesiones anteriores.</li>
</ul>
<h3 id="contenidos-para-la-sesion-presencial-del-17012024">Contenidos para la sesión presencial del 17/01/2024<a class="headerlink" href="#contenidos-para-la-sesion-presencial-del-17012024" title="Permanent link">&para;</a></h3>
<p>En la clase presencial (5 horas 🕒️ de duración), veremos cómo se implementa sobre nuestro código de la arquitectura transformer tanto un modelo de lengua basado en descodificador <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/lmgpt.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> como un modelo de reconocimiento de entidades nombradas <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/nerbert.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> basado en codificador. </p>
<p>Aprovecharemos para repasar algunos aspectos del código de sesiones anteriores y relacionar los aspectos teóricos con los prácticos. Presentaremos también la práctica que tienes que entregar para este bloque de la asignatura.</p>
<h2 id="cuarta-sesion-19-de-enero-de-2024">Cuarta sesión (19 de enero de 2024)<a class="headerlink" href="#cuarta-sesion-19-de-enero-de-2024" title="Permanent link">&para;</a></h2>
<p>Esta cuarta sesión es realmente la primera y única sesión del tema de voz. Mira la página sobre <a href="../speech/">voz</a> para ver los contenidos previos a esta sesión.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.c011b7c0.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.7389ff0e.min.js"></script>
      
    
  </body>
</html>