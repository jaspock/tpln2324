# Architectures for written-text processing

En este bloque se aborda el estudio de algunos modelos neuronales utilizados para procesar textos. El profesor de este bloque es Juan Antonio P√©rez Ortiz. El bloque comienza con un repaso del funcionamiento del regresor log√≠stico, que nos servir√° para asentar los conocimientos necesarios para entender posteriores modelos. A continuaci√≥n se estudia con cierto nivel de detalle *skip-grams*, uno de los algoritmos para la obtenci√≥n de *embeddings* incontextuales de palabras. Despu√©s se repasa el funcionamiento de las arquitecturas neuronales *feedforward* y se estudia su aplicaci√≥n a modelos de lengua. El objetivo √∫ltimo es abordar el estudio de la arquitectura m√°s importante de los sistemas actuales de procesamiento de textos: el transformer. Una vez estudiadas estas arquitecturas, finalizaremos con un an√°lisis del funcionamiento de los modelos preentrenados (modelos fundacionales), en general, y de los modelos de lengua, en particular.

Los materiales de clase complementan la lectura de algunos cap√≠tulos de un libro de texto ("Speech and Language Processing" de Dan Jurafsky y James H. Martin, borrador de la tercera edici√≥n, disponible online) con anotaciones realizadas por el profesor.

## Primera sesi√≥n (20 de diciembre de 2023)

### Contenidos a preparar antes de la sesi√≥n del 20/12/2023

Las actividades a realizar antes de esta clase son:

- Lectura y estudio de los contenidos de [esta p√°gina](https://dlsi.ua.es/~japerez/materials/transformers/regresor/) sobre regresi√≥n log√≠stica. Como ver√°s, la p√°gina te indica qu√© contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo prop√≥sito es ayudarte a entender los conceptos clave del cap√≠tulo. Despu√©s, realiza una segunda lectura del cap√≠tulo del libro. En total, esta parte deber√≠a llevarte unas 3 horas üïíÔ∏è de trabajo.
- Visionado y estudio de los tutoriales en v√≠deo de esta [playlist oficial de PyTorch](https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN).  Estudia al menos los 4 primeros v√≠deos (‚ÄúIntroduction to PyTorch‚Äù, ‚ÄúIntroduction to PyTorch Tensors‚Äù, ‚ÄúThe Fundamentals of Autograd‚Äù y ‚ÄúBuilding Models with PyTorch‚Äù). En total, esta parte deber√≠a llevarte unas 2 horas üïíÔ∏è de trabajo.
- Tras acabar con las dos partes anteriores, realiza este [test de evaluaci√≥n](https://forms.gle/V3U9MTHo7c9DNhkc6) de estos contenidos. Son pocas preguntas y te llevar√° unos minutos.

### Contenidos para la sesi√≥n presencial del 20/12/2023

En la clase presencial (2,5 horas üïíÔ∏è de duraci√≥n), veremos c√≥mo se implementa un regresor log√≠stico en PyTorch siguiendo la implementaci√≥n de un regresor log√≠stico binario y de uno multinomial que se comentan en [este apartado](https://dlsi.ua.es/~japerez/materials/transformers/implementacion/#codigo-para-un-regresor-logistico-y-uno-multinomial).

La idea es que vayas creando una serie de notebooks en Google Colab en los que comentes cada uno de los programas que vamos a ir viendo. En la √∫ltima clase se presentar√° una pr√°ctica m√°s avanzada que implicar√° modificar el c√≥digo del transformer.

Assignments before class of Dec 20, 2023: this class will have two parts taught by different teachers; therefore, your assignments will deal with two different topics; firstly, read the contents of section x; after that, complete this test (deadline: 23:59 CET, Dec 19 2023); secondly, read the materials related to logistic regressors and PyTorch linked in this ![section](text.md#contenidos-a-preparar-antes-de-la-sesion-del-20/12/2023) and then complete [this test](https://forms.gle/V3U9MTHo7c9DNhkc6) (same deadline: 23:59 CET, Dec 19 2023)